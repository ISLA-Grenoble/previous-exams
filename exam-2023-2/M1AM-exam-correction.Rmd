---
documentclass: article
papersize: a4
geometry: top=1.5cm, bottom=2cm, left=1.5cm, right=1.5cm
fontsize: 11pt
output:
  pdf_document:
    extra_dependencies: enumitem
    number_sections: yes
    toc: no
    keep_tex: no
    includes:
      in_header: M1AM-exam-preamble.tex
      before_body: M1AM-exam-header.tex
  html_document:
    toc: no
    df_print: paged
---

<!-- see help at https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html -->

```{r setup, include=FALSE, message=FALSE}
#see full list >knitr::opts_chunk$get()
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
```
This exam is composed of four parts.

**All students should do Part 1**. 

You should then choose to answer only **two parts out of the remaining three**. 

> For example, you can do just Part 1, Part 2, and Part 4. 

> Or maybe Part 1, Part 3, and Part 4.

> And so on.

The maximum final grade is 20.0 points.

Be sure to **read through all of the questions** before starting to solve the exam. 

Good luck!

\newpage

\section*{$\blacktriangleright$~Part 1: Multiple choice questions (8.0 points)}
For each question, cross the letter corresponding to the correct answer. 

Each question has **exactly one correct** answer.

\subsection*{\small--~Question 1: Robustness to outliers (credits to EPFL CS-433) (1.0 points)}
We consider a classification problem on linearly separable data. Our dataset had an outlier -- a point that is very far from the other datapoints in distance. We trained the linear discriminant analysis (LDA), logistic regression and 1-nearest-neighbour classifiers on this dataset. We tested trained models on a test set that comes from the same distribution as training set, but doesnâ€™t have any outlier points. After that we removed the outlier and retrained our models.

After retraining, which classifier will **not change** its decision boundary around the test points.
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] Logistic regression.
\item[{\color{blue}\textbf{(B)}}] 1-nearest-neighbors classifier.
\item[(C)] LDA.
\item[(D)] None of them.
\end{itemize}

\subsection*{\small--~Question 2: Bias-variance decomposition (credits to EPFL CS-433) (1.0 points)}
Consider a regression model where data $(x, y)$ is generated by input $x \in \mathbf{R}$ uniformly sampled between $[0, 1]$ and $y = x + \varepsilon$, where $\varepsilon$ is random noise with mean 0 and variance 1. Two models are carried out for regression: model $\mathcal{A}$ is a trained quadratic function $g_{\mathcal{A}}(x, \boldsymbol{\beta}) = \beta_0 + \beta_1 x + \beta_2 x^2$ and model $\mathcal{B}$ is a constant function $g_{\mathcal{B}}(x) = \tfrac{1}{2}$. 

Compared to model $\mathcal{B}$, model $\mathcal{A}$ has
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] Higher bias, higher variance.
\item[{\color{blue}\textbf{(B)}}] Lower bias, higher variance.
\item[(C)] Higher bias, lower variance.
\item[(D)] Lower bias, lower variance.
\end{itemize}

\subsection*{\small--~Question 3: Linear regression (credits to EPFL CS-433) (1.0 points)}
Assume we are doing linear regression with mean-squared loss and L2-regularization on four one-dimensional data points. Our prediction model can be written as $f(x) = a x + b$ and the optimization problem can be written as
$$
a^\star, b^\star = \underset{a, b}{\text{argmin}}~\sum_{i = 1}^4 \Big(y_i - f(x_i)\Big)^2 + \lambda a^2
$$
Assume that our data points $(x_i, y_i)$ are $\{(-2, 1), (-1, 3), (0, 2), (3, 4)\}$.

What is the optimal value for the bias, $b^\star$?
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] Depends on the value of $\lambda$.
\item[(B)] 3
\item[{\color{blue}\textbf{(C)}}] 2.5
\item[(D)] None of the above answers.
\end{itemize}

\subsection*{\small--~Question 4: PCA (credits to EPFL CS-433) (1.0 points)}
Which of the following transformations to a data matrix $\mathbf{X}$ will affect the principal components obtained through PCA?
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] Adding a constant value to all elements of $\mathbf{X}$.
\item[{\color{blue}\textbf{(B)}}] Multiplying one of the features of $\mathbf{X}$ by a constant.
\item[(C)] Adding an extra feature to $\mathbf{X}$ (i.e. an extra column) that is constant across all data points.
\item[(D)] None of the above answers.
\end{itemize}

\subsection*{\small--~Question 5: Ridge regularization (credits to EPFL CS-433) (1.0 points)}
Assume we have $N$ training samples $(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_N, y_N)$ where each $\mathbf{x}_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$. 

For $\lambda \geq 0$, we consider the following loss function:
$$
\mathcal{L}_\lambda(\boldsymbol{\beta}) = \dfrac{1}{N}\sum_{i = 1}^N \big(y_i - \mathbf{x}_i^\top \boldsymbol{\beta}\big)^2 + \lambda \|\boldsymbol{\beta}\|_2
$$
and let $C_\lambda = \min_{\boldsymbol{\beta} \in \mathbb{R}^p} \mathcal{L}_\lambda(\boldsymbol{\beta})$ denote the optimal loss value. Which of the following statements is **true**?
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] $C_\lambda$ is a non-increasing function of $\lambda$.
\item[(B)] For $\lambda = 0$, the loss $\mathcal{L}_0$ is non-convex and might have several minimizers.
\item[(C)] $C_\lambda$ is a non-decreasing function of $\lambda$.
\item[{\color{blue}\textbf{(D)}}] None of the above statements are true.
\end{itemize}

\subsection*{\small--~Question 6: Logistic regression (credits to EPFL CS-433) (1.0 points)}
Consider the logistic regression loss $\mathcal{L}: \mathbb{R}^p \to \mathbb{R}$ for a binary classification task with data $(\mathbf{x}_i, y_i) \in \mathbb{R}^p \times \{0, 1\}$:
$$
\mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{N}\sum_{i = 1}^N \left(\log\big(1 + e^{\mathbf{x}_i^\top \boldsymbol{\beta}}\big) - y_i \mathbf{x}_i^\top \boldsymbol{\beta}\right)
$$
Which of the following is a gradient of the loss $\mathcal{L}$?
\begin{itemize}
 \renewcommand{\labelitemi}{\scriptsize$\square$}
\item[(A)] $\nabla \mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N \left(\mathbf{x}_i \dfrac{e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}{1 + e^{\mathbf{x}_i^\top \boldsymbol{\beta}}} - y_i \mathbf{x}_i^\top \boldsymbol{\beta}\right)$
\item[(B)] $\nabla \mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N \mathbf{x}_i\left(y_i - \dfrac{e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}{1 + e^{\mathbf{x}_i^\top \boldsymbol{\beta}}}\right)$
\item[{\color{blue}\textbf{(C)}}] $\nabla \mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{N}\displaystyle\sum_{i = 1}^N \mathbf{x}_i\left(\dfrac{1}{1 + e^{-\mathbf{x}_i^\top \boldsymbol{\beta}}} - y_i\right)$
\item[(D)] None of the above.
\end{itemize}

\subsection*{\small--~Question 7: Linear regression (credits to Berkeley  CS-189) (1.0 points)}
In linear regression, we model $p(y \mid \mathbf{x}) \sim \mathcal{N}(\beta^\top \mathbf{x} + \beta_0, \sigma^2)$. The irreducible error in this model is
\begin{itemize}
\item[{\color{blue}\textbf{(A)}}] $\sigma^2$
\item[(B)] $\mathbb{E}[y \mid \mathbf{x}]$
\item[(C)] $\mathbb{E}[(y - \mathbb{E}[y \mid \mathbf{x}])^2 \mid \mathbf{x}]$
\item[(D)] None of the above.
\end{itemize}

\subsection*{\small--~Question 8: Classifier boundary (credits to EPFL CS-189) (1.0 points)}
Which of these classifiers could have generated the decision boundary here below
\begin{figure}[h]
\centering
\includegraphics[width=0.45\columnwidth]{figure.pdf}
\end{figure}
\begin{itemize}
\item[(A)] Logistic regression
\item[{\color{blue}\textbf{(B)}}] 1-NN
\item[(C)] Quadratic discriminant analysis
\item[(D)] None of the above.
\end{itemize}

\newpage
\section*{$\blacktriangleright$~Part 2: Weighted linear regression (6.0 points)}
Suppose we have a regression dataset with $N$ pairs $(\mathbf{x}_i, y_i)$ where $\mathbf{x}_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$. We wish to fit a linear model $f(\mathbf{x}_i) = \boldsymbol{\beta}^\top \tilde{\mathbf{x}}_i$ where $\boldsymbol{\beta}$ is a vector with entries $\beta_0, \beta_1, \dots, \beta_p$ and $\tilde{\mathbf{x}}_i^\top = [1~\mathbf{x}_i^\top]$.

Suppose we minimize the following cost function:
$$
\mathcal{L}(\boldsymbol{\beta}) = \dfrac{1}{2}\sum_{i = 1}^N w_i \Big(y_i - \boldsymbol{\beta}^\top \tilde{\mathbf{x}}_i\Big)^2
$$
where $w_i > 0$ are known real-valued scalars.
\begin{enumerate}
\item[(a)] Calculate the gradient of the loss function and make it equal to zero. You should write an expression in matrix-vector form similar to the expressions for least-squares given in the lectures. \textbf{(2.5 points)}
\item[(b)] Discuss the conditions under which the solution $\beta^\star$ is unique. \textbf{(1.5 points)}
\item[(c)] Assuming that these conditions hold, write down the expression for the unique solution. \textbf{(1.0 points)}
\item[(d)] Interpret the role of the weights $w_i$ in the results of the regression. In other words, explain how adding them to the usual linear regression model can be useful for certain situations. \textbf{(1.0 points)}
\end{enumerate}

\newpage
\section*{$\blacktriangleright$~Part 3: Principal component analysis (6.0 points)}
We consider the dataset \code{cars04}, which describes several properties of
different car models in the market in 2004. Each observation (i.e. car) is described by 11 features (i.e. properties) listed in Table 1.

\begin{table}[!hbt]
\begin{tabular}{ll}
Variable & Meaning\\
\hline
\code{Retail}  & Builder recommended price(US\$)\\
\code{Dealer}  & Seller price (US\$)\\
\code{Engine}  & Motor capacity  (liters)\\
\code{Cylinders}  & Number of cylinders in the motor\\
\code{Horsepower}  &Engine power\\
\code{CityMPG}  & Consumption in city (Miles or gallon; proportional to km/liter)\\
\code{HighwayMPG}  & Consumption on roadway  (Miles or gallon)\\
\code{Weight}  & Weight (pounds)\\
\code{Wheelbase}  & Distance between front and rear wheels (inches)\\
\code{Length}  & Length (inches)\\
\code{Width}   & Width  (inches)\\
\hline
\end{tabular}
\caption{Variable list for \code{cars04}}
\label{tab:cars04:varlist}
\end{table}


```{r, echo=FALSE}
## Load the "cars04" dataset
cars04 <- read.csv("cars-fixed04.csv")[,8:18]
```

The aim of this exercise is to summarize and to interpret the data \code{cars04} using PCA. Using `R` we run the following instruction:
```{r}
cars04.pca <- prcomp(cars04, scale=TRUE)
summary(cars04.pca)
```
\begin{enumerate}
\item[(a)] What is the effect of the argument \code{scale=TRUE} in the result of the PCA? \textbf{(1.5 points)}

\textit{The command \code{scale=TRUE} forces the variance of each column of the data matrix to be equal to one. This ensures that all predictors are comparable and avoids distortions in one direction.}

\item[(b)] Are the first two principal components enough to summarize most of the information (i.e. variance) of the dataset? Justify in terms of the proportion of the total variance that they represent. \textbf{(1.5 points)}

\textit{The first two principal components sum 80\% of the total variance of the dataset, which is quite high. Furthermore, we note that adding a 3rd component would only explain 7\% more of the variance, which we could argue as being too small to justify adding it.}

\end{enumerate} 
Principal components are linear combinations of the 11 variables. 
The coefficients of the first 2 principal components on the 11 feature are
```{r}
cars04.pca$rotation[,1:2]
```
\begin{enumerate}[resume]
\item[(c)] What would be a good interpretation for these new variables in terms of the initial features of the dataset? \textbf{(1.0 points)}
\end{enumerate} 

*PC1 represents features related to the motor of the cars. Positive values of PC1 describe cars with larger and more powerful engines (large coefficients for \code{Engine} and \code{Cylinders}), which tend to consume more energy, and negative values of PC1 relate to cars that are more energy efficient (large coefficients of \code{CityMPG} and \code{HighwayMPG}).*

*In PC2, large positive values relate to cars with big dimensions (\code{Wheelbase} and \code{Length}) and negative values represent expensive cars (\code{Retail} and \code{Dealer}).*

Figure \ref{fig:cars04:PCA} shows the projection of the dataset on its first two principal components.

\begin{enumerate}[resume]
\item[(d)] Interpret each quadrant of the figure. \textbf{(1.0 points)}

\textit{Wenote from the plot in Figure 1A that the efficiency of a car is in completely opposition to the size of its motor. Also, more expensive cars tend to be more compact, lighter, and with more horsepower.}

\textit{The quadrants of Figure 1B are directly related to the explanations given in item (C).}

\item[(e)] Can you describe which kind of car Audi RS 6, Ford Expedition 4.6 XLT and Nissan Sentra 1.8 are? \textbf{(1.0 points)}

\textit{
-- Audi RS 6: It is an expensive car which is not very energy efficient. It is also not very heavy and rather compact.
}

\textit{
-- Ford Expedition 4.6 XLT: It is a big and heavy car which is not very energy efficient. 
}

\textit{
-- Nissan Sentra 1.8: It is a small, compact and energy efficient car. It is more expensive than the Ford Expedition but cheaper than Audi RS 6.
}
\end{enumerate} 

```{r, echo=FALSE, eval=FALSE}
lambda = cars04.pca$sdev^2
p = dim(cars04)[2]

x = (-100:100)/100
y = sqrt(1-x^2)
P = cars04.pca$rotation
for(i in 1:p) P[,i] = P[,i] * sqrt(lambda[i])

plot(P[,1],P[,2], xlab="Axis 1", ylab = "Axis 2", xlim=c(-1,1), ylim=c(-1,1))
abline(h=0, v=0)
lines(x, y)
lines(x, -y)
text(P[, 1], P[, 2]-0.05, names(cars04))
```


\begin{figure}[htp]
  \centering
  \begin{subfigure}[b]{0.47\textwidth}
```{r, echo=FALSE, fig.width=5, fig.height=5}
x <- (-100:100)/100
y <- sqrt(1-x^2)
lambda = cars04.pca$sdev^2
P = cars04.pca$rotation
for(i in 1:NCOL(P)) 
  P[,i] <- P[,i] * sqrt(lambda[i])

plot(P[,1], P[,2], xlab="Axis 1", ylab = "Axis 2", xlim=c(-1,1), ylim=c(-1,1))
abline(h=0, v=0)
lines(x, y)
lines(x, -y)
text(P[, 1]+0.1*rep(c(1,-1), length=NCOL(cars04)), 
     P[, 2]-0.1*rep(c(1,-1), length=NCOL(cars04)), names(cars04))

```     
     \caption{Variable space}
     \label{fig:cars04:PCA:variable}
  \end{subfigure}
  \hspace{5pt}
  \begin{subfigure}[b]{0.47\textwidth}
```{r, echo=FALSE, fig.width=5, fig.height=5}
somerows <- c("Audi RS 6", "Ford Expedition 4.6 XLT", "Nissan Sentra 1.8")
samplrows <- sample(setdiff(rownames(cars04), somerows), 25)
plot(cars04.pca$x[, 1], cars04.pca$x[, 2], xlab="PC1", ylab="PC2", type="n", xlim=c(-8,8), ylim=c(-4,4))
abline(h=0, v=0)
text(cars04.pca$x[samplrows, 1], cars04.pca$x[samplrows, 2], samplrows, cex=0.6, col="grey40")
text(cars04.pca$x[somerows, 1], cars04.pca$x[somerows, 2], somerows, cex=1.05)
```
    \caption{Individual space}
     \label{fig:cars04:PCA:individual}
  \end{subfigure}
\caption{Principal component representation in the first plane of the variable and of the sample spaces.}
  \label{fig:cars04:PCA}
\end{figure}

\newpage
\section*{$\blacktriangleright$~Part 4: Spectral community detection (6.0 points)}
In this part, you will be asked a few questions related to the content of the article that I've asked you to read before the final exam: "*Modularity and community structure in networks*" by Mark Newman.

\begin{enumerate}
\item[(a)] Give two examples of applications of community detection in networks in real life. \textbf{(1.0 points)}

\textit{
-- Community detection can be used to detect groups within the worldwide web that might correspond to sets of web pages on related topics. 
}

\textit{
-- Community detection can be used to detect groups within social networks that might correspond to social units or communities.
}

\item[(b)] Give the definition of modularity and explain why it is a good target quantity when investigating community structures in networks. \textbf{(1.5 points)}

\textit{The modularity is, up to a multiplicative constant, the number of edges falling within groups minus the expected number in an equivalent network with edges placed at random.}

\textit{Modularity can be either positive or negative, with positive values indicating the possible presence of community structure. Thus, one can search for community structure pre- cisely by looking for the divisions of a network that have positive, and preferably large, values of the modularity.}

\item[(c)] What is the interpretation in terms of community detection when the modularity matrix of a network has no positive eigenvalues? \textbf{(1.0 points)}

\textit{When the modularity matrix has no positive eigenvalues, it indicates that the graph can't be satisfactorily split into two communities and, therefore, should be left as it is.}

\item[(d)] What information does the magnitude of each coordinate of the leading eigenvector of the modularity matrix convey regarding the split of a network into two communities? \textbf{(1.5 points)}

\textit{The magnitudes of the leading eigenvector indicate which vertices corresponding make the largest contributions to the modularity. These values indicate how confident/certain we can be of the the group to which the vertex has been assigned to.}

\item[(e)] What is the procedure proposed by the author of the paper for splitting a network into more than two communities using his spectral algorithm? Is it capable of proposing a split into three communities? Why? \textbf{(1.0 points)}

\textit{The paper proposed a bissection procedure, where each group of the graph is further divided into new groups. Thanks to its natural stopping criterium (i.e. no positive eigenvalues when the graph should no longer be split) the algorithm can indeed detect three communities: it would first detect two, then one of the sub-groups would be further split into two whereas the other one would not.}

\end{enumerate}
